title: 'Tutorial 8: Deep Autoencoders'
author: Phillip Lippe
created: 2021-07-12
license: CC BY-SA
tags:
- Image
description: 'In this tutorial, we will take a closer look at autoencoders (AE).

  Autoencoders are trained on encoding input data such as images into a smaller feature
  vector,

  and afterward, reconstruct it by a second neural network, called a decoder.

  The feature vector is called the "bottleneck" of the network as we aim to compress
  the input data into a smaller amount of features.

  This property is useful in many applications, in particular in compressing data
  or comparing images on a metric beyond pixel-level comparisons.

  Besides learning about the autoencoder framework, we will also see the "deconvolution"

  (or transposed convolution) operator in action for scaling up feature maps in height
  and width.

  Such deconvolution networks are necessary wherever we start from a small feature
  vector

  and need to output an image of full size (e.g. in VAE, GANs, or super-resolution
  applications).

  This notebook is part of a lecture series on Deep Learning at the University of
  Amsterdam.

  The full list of tutorials can be found at https://uvadlc-notebooks.rtfd.io.

  '
accelerator:
- CPU
- GPU
environment:
- torchvision==0.15.2+cu118
- torchmetrics==1.2.1
- torch==2.0.1+cu118
- pytorch-lightning==2.0.9.post0
- numpy==1.26.4
- matplotlib==3.9.1
- tensorboard==2.17.0
- seaborn==0.13.2
published: '2024-07-23T12:53:38.906591'
