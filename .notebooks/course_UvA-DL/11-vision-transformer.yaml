title: 'Tutorial 11: Vision Transformers'
author: Phillip Lippe
created: 2021-08-21
license: CC BY-SA
description: 'In this tutorial, we will take a closer look at a recent new trend:
  Transformers for Computer Vision.

  Since [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) successfully
  applied a Transformer on a variety of image recognition benchmarks, there have been
  an incredible amount of follow-up works showing that CNNs might not be optimal architecture
  for Computer Vision anymore.

  But how do Vision Transformers work exactly, and what benefits and drawbacks do
  they offer in contrast to CNNs?

  We will answer these questions by implementing a Vision Transformer ourselves, and
  train it on the popular, small dataset CIFAR10.

  We will compare these results to popular convolutional architectures such as Inception,
  ResNet and DenseNet.

  This notebook is part of a lecture series on Deep Learning at the University of
  Amsterdam.

  The full list of tutorials can be found at https://uvadlc-notebooks.rtfd.io.

  '
tags:
- Image
accelerator:
- CPU
- GPU
environment:
- pytorch-lightning==2.0.9.post0
- torchmetrics==1.2.1
- seaborn==0.13.2
- torchvision==0.15.2+cu118
- tensorboard==2.17.0
- matplotlib==3.9.1
- torch==2.0.1+cu118
- numpy==1.26.4
published: '2024-07-23T17:49:04.764696'
